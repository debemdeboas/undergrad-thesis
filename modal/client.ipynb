{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Generator, Iterator, Literal, Union\n",
    "\n",
    "import httpx\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "from llama_index.core.chat_engine.types import BaseChatEngine\n",
    "from llama_index.core.indices.base import BaseIndex\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser, MarkdownNodeParser\n",
    "from llama_index.core.storage import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.vllm import Vllm\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.DEBUG, format=\"[%(filename)s:%(lineno)s - %(funcName)s() ] %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Valves(BaseModel):\n",
    "    \"\"\"\n",
    "    Configuration options for the pipeline.\n",
    "    These options can be set through the OpenWebUI interface.\n",
    "    \"\"\"\n",
    "\n",
    "    temperature: float = 0.5\n",
    "\n",
    "    hf_token: str = \"\"\n",
    "\n",
    "    embed_model: str = \"BAAI/bge-m3\"  # nomic-ai/nomic-embed-text-v1.5\n",
    "    embed_model_trust_remote_code: bool = False\n",
    "    embed_cuda: bool = False\n",
    "\n",
    "    oai_like_api_base: str = \"https://debemdeboas-tcc--vllm-openai-compatible-model-serve.modal.run/v1\"\n",
    "    oai_like_api_key: str = \"super-secret-token\"\n",
    "    oai_like_model: str = \"/models/meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "    system_prompt: str = (\n",
    "        \"Act as a teacher assistant and answer questions using the provided context.\\n\"\n",
    "        \"Your goal is to help students and teachers by providing cohesive and correct responses based on educational material, while applying guided learning techniques. Give examples and cite the context whenever possible.\\n\"\n",
    "        \"Don't mention 'according to the context' or anything related to that, ever.\\n\\n\"\n",
    "        \"## Instructions\\n\"\n",
    "        \"1. External Information: Use external information from the vector database to answer questions. Select the most relevant and reliable information available.\\n\"\n",
    "        \"2. Guided Learning Techniques: Avoid giving direct answers. Instead, guide the user through the learning process, encouraging critical thinking and discovery.\\n\"\n",
    "        \"3. Coherent and Correct Responses: Ensure that all responses are coherent and correct, strictly following the educational material provided.\\n\"\n",
    "        \"4. Inference Capability: Use your skills to accurately deduce and infer information.\\n\"\n",
    "        \"5. User-Friendly Interface: Be easy to use and access. Provide clear and well-structured responses suitable for a web interface.\\n\"\n",
    "        \"6. Value Addition: Add value for both students and teachers. Offer useful insights, pedagogical guidance, and support the teaching-learning process.\\n\"\n",
    "        \"7. Best-effort: The user is a beginner, and may use terms incorrectly or in other languages. Do your best to understand what they mean.\\n\\n\"\n",
    "        \"## User Interaction\\n\"\n",
    "        \"- Interactive Guidance: Ask the user if they would like more details or additional examples.\\n\"\n",
    "        \"- Encourage Exploration: Motivate users to explore more about the topic by suggesting additional resources or related questions.\\n\\n\"\n",
    "        \"## Additional Information\\n\"\n",
    "        \"- Utilize the context provided in the vector database to enrich your responses.\\n\"\n",
    "        \"- Ensure your answers are always up-to-date and based on the most recent information available.\\n\\n\"\n",
    "        \"Your mission is to provide a rich and interactive learning experience, helping students and teachers achieve their educational goals efficiently and effectively.\\n\"\n",
    "    )\n",
    "\n",
    "    learning_analytics_api: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valves = Valves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SentenceTransformer.py:113 - __init__() ] Load pretrained SentenceTransformer: BAAI/bge-m3\n",
      "[connectionpool.py:1055 - _new_conn() ] Starting new HTTPS connection (1): huggingface.co:443\n",
      "[connectionpool.py:549 - _make_request() ] https://huggingface.co:443 \"HEAD /BAAI/bge-m3/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "[connectionpool.py:549 - _make_request() ] https://huggingface.co:443 \"HEAD /BAAI/bge-m3/resolve/main/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "[connectionpool.py:549 - _make_request() ] https://huggingface.co:443 \"HEAD /BAAI/bge-m3/resolve/main/README.md HTTP/1.1\" 200 0\n",
      "[connectionpool.py:549 - _make_request() ] https://huggingface.co:443 \"HEAD /BAAI/bge-m3/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "[connectionpool.py:549 - _make_request() ] https://huggingface.co:443 \"HEAD /BAAI/bge-m3/resolve/main/sentence_bert_config.json HTTP/1.1\" 200 0\n",
      "[connectionpool.py:549 - _make_request() ] https://huggingface.co:443 \"HEAD /BAAI/bge-m3/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "[connectionpool.py:549 - _make_request() ] https://huggingface.co:443 \"HEAD /BAAI/bge-m3/resolve/main/model.safetensors HTTP/1.1\" 404 0\n",
      "[connectionpool.py:549 - _make_request() ] https://huggingface.co:443 \"HEAD /BAAI/bge-m3/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "[connectionpool.py:549 - _make_request() ] https://huggingface.co:443 \"GET /api/models/BAAI/bge-m3/revision/main HTTP/1.1\" 200 3322\n",
      "[SentenceTransformer.py:231 - __init__() ] 2 prompts are loaded, with the keys: ['query', 'text']\n",
      "[simple_kvstore.py:96 - from_persist_path() ] Loading llama_index.core.storage.kvstore.simple_kvstore from ../llama_index/persist/docstore.json.\n",
      "[local.py:298 - __init__() ] open file: /home/debem/personal/pucrs/tcc/cur/undergrad-thesis/modal/../llama_index/persist/docstore.json\n",
      "[simple_kvstore.py:96 - from_persist_path() ] Loading llama_index.core.storage.kvstore.simple_kvstore from ../llama_index/persist/index_store.json.\n",
      "[local.py:298 - __init__() ] open file: /home/debem/personal/pucrs/tcc/cur/undergrad-thesis/modal/../llama_index/persist/index_store.json\n",
      "[simple.py:168 - from_persist_path() ] Loading llama_index.core.graph_stores.simple from ../llama_index/persist/graph_store.json.\n",
      "[local.py:298 - __init__() ] open file: /home/debem/personal/pucrs/tcc/cur/undergrad-thesis/modal/../llama_index/persist/graph_store.json\n",
      "[local.py:298 - __init__() ] open file: /home/debem/personal/pucrs/tcc/cur/undergrad-thesis/modal/../llama_index/persist/property_graph_store.json\n",
      "[simple.py:405 - from_persist_path() ] Loading llama_index.core.vector_stores.simple from ../llama_index/persist/image__vector_store.json.\n",
      "[local.py:298 - __init__() ] open file: /home/debem/personal/pucrs/tcc/cur/undergrad-thesis/modal/../llama_index/persist/image__vector_store.json\n",
      "[simple.py:405 - from_persist_path() ] Loading llama_index.core.vector_stores.simple from ../llama_index/persist/default__vector_store.json.\n",
      "[local.py:298 - __init__() ] open file: /home/debem/personal/pucrs/tcc/cur/undergrad-thesis/modal/../llama_index/persist/default__vector_store.json\n",
      "[loading.py:63 - load_indices_from_storage() ] Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "index: BaseIndex\n",
    "documents: list\n",
    "engine: BaseChatEngine\n",
    "storage_context: StorageContext\n",
    "\n",
    "persist_dir = \"../llama_index/persist\"\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    valves.embed_model, trust_remote_code=valves.embed_model_trust_remote_code, device=\"cuda\"\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = OpenAILike(\n",
    "    model=valves.oai_like_model,\n",
    "    api_base=valves.oai_like_api_base,\n",
    "    api_key=valves.oai_like_api_key,\n",
    "    system_prompt=valves.system_prompt,\n",
    "    temperature=valves.temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = CondensePlusContextChatEngine.from_defaults(\n",
    "    index.as_retriever(),\n",
    "    context_prompt=valves.system_prompt\n",
    "    + (\n",
    "        \"Here are the relevant documents for the context:\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\"\n",
    "        \"\\nPlease answer in the same language as the question.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'engine' is not defined"
     ]
    }
   ],
   "source": [
    "response = engine.chat(\"hi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
